{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Part 2: IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, the task is to code a Named Entity Recognizer (NER) application in Python using the CRFsuite library.\n",
    "\n",
    "It is recommended you complete the Named_Entity_Extraction_Tutorial.ipynb tutorial before attemping this.\n",
    "\n",
    "Your tasks for this assignment are to:\n",
    "1. Build a NER classifier following the tutorial.\n",
    "2. Improve the performance of your NER classifier.\n",
    "3. Answer three written assignments.\n",
    "\n",
    "* Write answers in this notebook file, and upload the file to Wattle submission site. **Please rename and submit jupyter notebook file (Assignment5.ipynb) to your_uid.ipynb (e.g. u6000001.ipynb) with your written answers therein**. Do not upload any other files to Wattle except this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Question 1 (2 points) Build a NER model <a id='Task1'></a> </span>\n",
    "### Part A (1.5 marks)\n",
    "\n",
    "* Build a NER model using the train and test data files.\n",
    "* You can use the code provided in [tutorial sheet](Named_Entity_Extraction_Tutorial.ipynb) \n",
    "* Try changing the feature extraction, model hyper parameters, or other settings in order to improve your model performance.\n",
    "* Marks will be awarded based on how well your model performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.1\n",
      "Libraries succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('Libraries succesfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, # This feature is constant for all words.\n",
    "        'word.lower()': word.lower(), # This feature is the word, ignoring case.\n",
    "        'word[-2:]': word[-2:], # This feature is the last two characters of the word (i.e. the suffix).\n",
    "    }\n",
    "    if i == 0:\n",
    "        features['BOS'] = True # Mark the beginning of sentence.\n",
    "        \n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True # Mark the end of sentence.\n",
    "\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            # Each line contains the position ID, the token, and (for the training set) the label.\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test data loaded succesfully!\n",
      "Feature Extraction done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0, 'word.lower()': 'también', 'word[-2:]': 'én', 'BOS': True},\n",
       " {'bias': 1.0, 'word.lower()': 'el', 'word[-2:]': 'el'},\n",
       " {'bias': 1.0, 'word.lower()': 'secretario', 'word[-2:]': 'io'},\n",
       " {'bias': 1.0, 'word.lower()': 'general', 'word[-2:]': 'al'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'asociación', 'word[-2:]': 'ón'},\n",
       " {'bias': 1.0, 'word.lower()': 'española', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'operadores', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'productos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'petrolíferos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'aurelio', 'word[-2:]': 'io'},\n",
       " {'bias': 1.0, 'word.lower()': 'ayala', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'ha', 'word[-2:]': 'ha'},\n",
       " {'bias': 1.0, 'word.lower()': 'negado', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'existencia', 'word[-2:]': 'ia'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'cualquier', 'word[-2:]': 'er'},\n",
       " {'bias': 1.0, 'word.lower()': 'tipo', 'word[-2:]': 'po'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'acuerdos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'sobre', 'word[-2:]': 're'},\n",
       " {'bias': 1.0, 'word.lower()': 'los', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'precios', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'afirmando', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'que', 'word[-2:]': 'ue'},\n",
       " {'bias': 1.0, 'word.lower()': 'únicamente', 'word[-2:]': 'te'},\n",
       " {'bias': 1.0, 'word.lower()': 'es', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'cotización', 'word[-2:]': 'ón'},\n",
       " {'bias': 1.0, 'word.lower()': 'internacional', 'word[-2:]': 'al'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'que', 'word[-2:]': 'ue'},\n",
       " {'bias': 1.0, 'word.lower()': 'pone', 'word[-2:]': 'ne'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'acuerdo', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'a', 'word[-2:]': 'a'},\n",
       " {'bias': 1.0, 'word.lower()': 'todos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'los', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'países', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': '.', 'word[-2:]': '.', 'EOS': True}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data loaded succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2simple_features) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2simple_features) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    \n",
    "\n",
    "# Explore the extracted features    \n",
    "sent2features(train_data[0], word2simple_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 0.3,   # coefficient for L1 penalty\n",
    "    'c2': 1e-2,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "\n",
    "print('Training done :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.74      0.81      0.77      1857\n",
      "       I-LOC       0.54      0.73      0.62       561\n",
      "      B-MISC       0.36      0.61      0.45       510\n",
      "      I-MISC       0.40      0.42      0.41      1199\n",
      "       B-ORG       0.71      0.84      0.77      2726\n",
      "       I-ORG       0.62      0.67      0.65      2056\n",
      "       B-PER       0.78      0.89      0.83      1662\n",
      "       I-PER       0.85      0.87      0.86      1581\n",
      "\n",
      "   micro avg       0.67      0.76      0.71     12152\n",
      "   macro avg       0.63      0.73      0.67     12152\n",
      "weighted avg       0.67      0.76      0.71     12152\n",
      " samples avg       0.08      0.08      0.08     12152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell should look something like this (but with different numbers)\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "      B-LOC       0.68      0.47      0.55      1084\n",
    "      I-LOC       0.52      0.25      0.34       325\n",
    "     B-MISC       0.54      0.11      0.19       339\n",
    "     I-MISC       0.54      0.22      0.32       557\n",
    "      B-ORG       0.76      0.51      0.61      1400\n",
    "      I-ORG       0.67      0.44      0.53      1104\n",
    "      B-PER       0.73      0.68      0.71       735\n",
    "      I-PER       0.78      0.82      0.80       634\n",
    "\n",
    "avg / total       0.68      0.48      0.55      6178\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B (0.5 marks)\n",
    "\n",
    "Briefly explain what changes to your model you tried and how these changes affected the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I increased the L1 penalty, decreased the L2 penalty and increased the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Written Part (3 points) </span>\n",
    "\n",
    "Answer briefly and concisely the following questions.\n",
    "Check [this](https://sourceforge.net/p/jupiter/wiki/markdown_syntax/#md_ex_lists) if you are not familiar with markdown syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 point)\n",
    "Think of three relevant baselines for the Named Entity Classification task.\n",
    "Provide answers using bullet list with 3 items. Give a short description of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1.5 point)\n",
    "How does Maximal Marginal Relevance (MMR) address redundancy issues? (0.5 point)\n",
    "\n",
    "How can you tell MMR that \"Sydney\" and \"Melbourne\" are cities? (0.5 points)\n",
    "\n",
    "How can you tell MMR that \"solar panels\" and \"photovoltaic cells\" have similar meaning? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Reference - https://www.aclweb.org/anthology/X98-1025.pdf\n",
    "\n",
    "\n",
    "The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in reranking retrieved documents and in selecting appropriate passages for text summarization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) MMR can tell that these two words or sentences in a document or corpuse are similar meaning as MMR uses cosine similarity through which we can know that this belongs to the cities group as there similarity of both being in cities group would be close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) MMR can tell that these two words are similar meaning as MMR uses cosine similarity which helps us to know whether the word has same meaning or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1 point)\n",
    "\n",
    "Imagine you are developing an extractive text summarization tool using HMM.\n",
    "\n",
    "What are the hidden states and the observations of the HMM model? (0.5 point)\n",
    "\n",
    "Which algorithm is used to compute the probability of a particular observation sequence? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)Reference - https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
    "\n",
    "The states of the stochastic processes are called hidden states whereas the observations is the result that we get after soving the markov hidden model using the hidden layers.\n",
    "\n",
    "b) Forward algorithm is used to compute the probability of a particular observation sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
